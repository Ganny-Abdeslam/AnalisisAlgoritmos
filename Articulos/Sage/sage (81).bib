@article{doi:10.1177/03946320231207514,
author = {Azzeddine Annan and Noureddine Raiss and El Harti Elmir and Abdelkarim Filali-Maltouf and Leila Medraoui and Hicham Oumzil},
title = {Revolutionizing antiretroviral therapy for human immunodeficiency virus/AIDS: A computational approach using molecular docking, virtual screening, and 3D pharmacophore building to address therapeutic failure and propose highly effective candidates},
journal = {International Journal of Immunopathology and Pharmacology},
volume = {37},
number = { },
pages = {03946320231207514},
year = {2023a},
doi = {10.1177/03946320231207514},
note = {PMID:37850462},
URL = {https://doi-org.crai.referencistas.com/10.1177/03946320231207514},
eprint = {https://doi-org.crai.referencistas.com/10.1177/03946320231207514},
abstract = {Objectives: In the context of human immunodeficiency virus (HIV) treatment, the emergence of therapeutic failures with existing antiretroviral drugs presents a significant challenge. This study aims to employ advanced molecular modeling techniques to identify potential alternatives to current antiretroviral agents. Methods: The study focuses on three essential classes of antiretroviral drugs: nucleoside reverse transcriptase inhibitors (NRTIs), non-nucleoside reverse transcriptase inhibitors (NNRTIs), and protease inhibitors (PIs). Computational analyses were performed on a database of 3,343,652 chemical molecules to evaluate their binding affinities, pharmacokinetic properties, and interactions with viral reverse transcriptase and protease enzymes. Molecular docking, virtual screening, and 3D pharmacophore modeling were utilized to identify promising candidates. Results: Molecular docking revealed compounds with high binding energies and strong interactions at the active sites of target enzymes. Virtual screening narrowed down potential candidates with favorable pharmacological profiles. 3D pharmacophore modeling identified crucial structural features for effective binding. Overall, two molecules for class 1, 7 molecules for class 2, and 2 molecules for class 3 were selected. These compounds exhibited robust binding affinities, interactions with target enzymes, and improved pharmacokinetic properties, showing promise for more effective HIV treatments in cases of therapeutic failures. Conclusion: The combination of molecular docking, virtual screening, and 3D pharmacophore modeling yielded lead compounds that hold potential for addressing HIV therapeutic failures. Further experimental investigations are essential to validate the efficacy and safety of these compounds, with the ultimate goal of advancing toward clinical applications in HIV management.}
}

@article{doi:10.3233/IA-2011-0005,
author = {Matteo Baldoni and Cristina Baroglio},
title = {A journey in Computational Logic in Italy},
journal = {Intelligenza Artificiale},
volume = {5},
number = {1},
pages = {67–69},
year = {2011b},
doi = {10.3233/IA-2011-0005},
URL = {https://doi-org.crai.referencistas.com/10.3233/IA-2011-0005},
eprint = {https://doi-org.crai.referencistas.com/10.3233/IA-2011-0005}
}

@article{doi:10.1243/09544062JMES2473,
author = {D I M Forehand and M P Cartmell},
title = {The implementation of an automated method for solution term-tracking as a basis for symbolic computational dynamics},
journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
volume = {225},
number = {1},
pages = {40–49},
year = {2011c},
doi = {10.1243/09544062JMES2473},
URL = {https://doi-org.crai.referencistas.com/10.1243/09544062JMES2473},
eprint = {https://doi-org.crai.referencistas.com/10.1243/09544062JMES2473},
abstract = {This article proposes that additional mathematical information, inherent and implicit within mathematical models of physical dynamic systems, can be extracted and visualized in a physically meaningful and useful manner as an adjunct to standard analytical modelling and solution. A conceptual methodology is given for a process of term-tracking within ordinary differential equation (ODE) models and solutions for engineering dynamics problems, and for a visualization based on a powerful new Mathematica implementation of the standard Tooltip graphical user interface facility. It is shown that the method is logical, generic, and unambiguous in its application, and that a useful visualization tool can be devised, and structured in such a way that the user can be given as much or as little information as is required to assimilate the problem to hand. The article shows by means of examples of code written expressly for the purpose that a term-tracking and visualization methodology can be constructed in a computationally effective manner within Mathematica and applied to a semi-automated variant of the method of multiple scales. It is implicitly obvious that this approach can therefore be applied to almost any algorithmic symbolic solution method, and therefore there could be physical applications which are potentially well beyond the chosen domain of non-linear engineering dynamics.}
}

@article{doi:10.1260/147807706778658801,
author = {Sergio Araya Goldberg},
title = {Computational Design of Parametric Scripts for Digital Fabrication of Curved Structures},
journal = {International Journal of Architectural Computing},
volume = {4},
number = {3},
pages = {99–117},
year = {2006d},
doi = {10.1260/147807706778658801},
URL = {https://doi-org.crai.referencistas.com/10.1260/147807706778658801},
eprint = {https://doi-org.crai.referencistas.com/10.1260/147807706778658801},
abstract = {This paper explores strategies for building toolchains to design, develop and fabricate architectural designs. It explains how complex curved structures can be constructed from flat standard panels. The hypothesis of this research is that by embedding ruled based procedures addressing generative, variational, iterative, and fabricational logics into early phases of design, both design techniques and digital fabrication methods can merge to solve a recurrent problem in contemporary architectural design, building double curved structures. Furthermore it achieves this using common fabrication methods and standard construction materials. It describes the processes of programming computational tools creating and developing designs to fabricate continuous complex curved structures. I describe this through a series of experiments, using parametric design environments and scripted functions, implementing certain techniques to fabricate these designs using rapid prototyping machines. Comparing different design and fabrication approaches I offer a discussion about universal application of programmed procedures into architectural design.}
}

@article{doi:10.1177/0735633120985108,
author = {Wen-Chin Hsu and Julie Gainsburg},
title = {Hybrid and Non-Hybrid Block-Based Programming Languages in an Introductory College Computer-Science Course},
journal = {Journal of Educational Computing Research},
volume = {59},
number = {5},
pages = {817–843},
year = {2021e},
doi = {10.1177/0735633120985108},
URL = {https://doi-org.crai.referencistas.com/10.1177/0735633120985108},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0735633120985108},
abstract = {Block-based programming languages (BBLs) have been proposed as a way to prepare students for learning to program in more sophisticated, text-based languages, such as Java. Hybrid BBLs add the ability to view and edit the block commands in auto-generated, text-based code. We compared the use of a non-hybrid BBL (Scratch), a hybrid BBL (Pencil Code), and no BBL across three sections of an introductory CS course that taught Java programming, to determine whether either type of BBL offered cognitive or affective advantages for learning Java. Students in the BBL groups were surveyed about their perceptions of each BBL in terms of ease of use and helpfulness in learning Java, and all three groups were compared on their performance in Java programming. The results showed that, in this introductory CS course, neither type of BBL offered an advantage in preparing students for learning Java. These results held regardless of the students’ level of Java knowledge prior to the course.}
}

@article{doi:10.1177/1071181321651154,
author = {May Jorella Lazaro and Sungho Kim and Yohan Kang and Myung Hwan Yun},
title = {Visual Search and Decluttering in Tactical Situation Displays: A Computational Modeling Approach},
journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
volume = {65},
number = {1},
pages = {1425–1431},
year = {2021f},
doi = {10.1177/1071181321651154},
URL = {https://doi-org.crai.referencistas.com/10.1177/1071181321651154},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1071181321651154},
abstract = {Clutter in tactical situation displays (TSD) is a persistent problem that affects pilots’ performance. Decluttering methods such as dimming, dotting, small-sizing and removal have been used in several display types to reduce clutter. This study aims to investigate the effects of different decluttering methods applied in TSD on pilots’ visual search performance. It also aims to uncover the basic psychophysical processes underlying the pilots’ visual search behavior through computational modeling. Data from fifteen Air-Force pilots showed that accuracy is higher and response time is faster when the TSD is decluttered, regardless of the technique. However, when the data was fitted into the hierarchical drift-diffusion model, it was revealed that among the techniques tested, dimming yielded the best search performance based on the model parameters. This study suggests that analyzing behavioral data through computational modeling may lead to better insights that are more practical and applicable in solving the issues in visual search in TSDs.}
}

@article{doi:10.1177/0954406216687790,
author = {Tianliang Lin and Qiang Chen and Haoling Ren and Ruoxi Lv and Chen Miao and Qihuai Chen},
title = {Computational fluid dynamics and experimental analysis of the influence of the energy recovery unit on the proportional relief valve},
journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
volume = {232},
number = {4},
pages = {697–705},
year = {2018g},
doi = {10.1177/0954406216687790},
URL = {https://doi-org.crai.referencistas.com/10.1177/0954406216687790},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0954406216687790},
abstract = {The overflow energy loss in relief valve, which is one of the main reasons leading to the low efficiency of the hydraulic system, had been considered to be impossible to solve. The principle of the overflow energy loss of the relief valve is analyzed and a novel method to reduce the overflow loss using an energy recovery unit, which can improve the return line pressure of the pilot proportional relief valve, is proposed. The influence of the energy recovery unit on the pressure control characteristics and steady-state flow force of the pilot proportional relief valve are discussed. The effects of the return line pressure on the distribution of the flow field and the pressure control characteristics are analyzed through computational fluid dynamics simulation and experiment. The results show that with the increase of the return line pressure, the displacement of the main valve spool increases and the reset spring force increases accordingly. While the steady-state flow force decreases dramatically with the increase of the return line pressure, which results in a smaller pressure differential the pressure differential can be reduced from 15% to 2.5%. It is also observed that the flow rate of the pilot proportional relief valve can be maintained at a certain value with a small oscillation and that the pilot proportional relief valve can release the redundant flow of hydraulic system. This verifies that the pilot proportional relief valve with the outlet connecting to the energy recovery unit to recovery the overflow energy loss cannot reduce the pressure control characteristics, but can achieve a better pressure control accuracy of the pilot proportional relief valve.}
}

@article{doi:10.1080/17470218.2015.1134602,
author = {Pavel Logačev and Shravan Vasishth},
title = {Understanding underspecification: A comparison of two computational implementations},
journal = {Quarterly Journal of Experimental Psychology},
volume = {69},
number = {5},
pages = {996–1012},
year = {2016h},
doi = {10.1080/17470218.2015.1134602},
note = {PMID:26960441},
URL = {https://doi-org.crai.referencistas.com/10.1080/17470218.2015.1134602},
eprint = {https://doi-org.crai.referencistas.com/10.1080/17470218.2015.1134602},
abstract = {Swets et al. (2008. Underspecification of syntactic ambiguities: Evidence from self-paced reading. Memory and Cognition, 36(1), 201–216) presented evidence that the so-called ambiguity advantage [Traxler et al. (1998). Adjunct attachment is not a form of lexical ambiguity resolution. Journal of Memory and Language, 39(4), 558–592], which has been explained in terms of the Unrestricted Race Model, can equally well be explained by assuming underspecification in ambiguous conditions driven by task-demands. Specifically, if comprehension questions require that ambiguities be resolved, the parser tends to make an attachment: when questions are about superficial aspects of the target sentence, readers tend to pursue an underspecification strategy. It is reasonable to assume that individual differences in strategy will play a significant role in the application of such strategies, so that studying average behaviour may not be informative. In order to study the predictions of the good-enough processing theory, we implemented two versions of underspecification: the partial specification model (PSM), which is an implementation of the Swets et al. proposal, and a more parsimonious version, the non-specification model (NSM). We evaluate the relative fit of these two kinds of underspecification to Swets et al.’s data; as a baseline, we also fitted three models that assume no underspecification. We find that a model without underspecification provides a somewhat better fit than both underspecification models, while the NSM model provides a better fit than the PSM. We interpret the results as lack of unambiguous evidence in favour of underspecification; however, given that there is considerable existing evidence for good-enough processing in the literature, it is reasonable to assume that some underspecification might occur. Under this assumption, the results can be interpreted as tentative evidence for NSM over PSM. More generally, our work provides a method for choosing between models of real-time processes in sentence comprehension that make qualitative predictions about the relationship between several dependent variables. We believe that sentence processing research will greatly benefit from a wider use of such methods.}
}

@article{doi:10.1177/00491241211031273,
author = {Ramina Sotoudeh and Paul DiMaggio},
title = {Coping With Plenitude: A Computational Approach to Selecting the Right Algorithm},
journal = {Sociological Methods & Research},
volume = {52},
number = {4},
pages = {1838–1882},
year = {2023i},
doi = {10.1177/00491241211031273},
URL = {https://doi-org.crai.referencistas.com/10.1177/00491241211031273},
eprint = {https://doi-org.crai.referencistas.com/10.1177/00491241211031273},
abstract = {Sociologists increasingly face choices among competing algorithms that represent reasonable approaches to the same task, with little guidance in choosing among them. We develop a strategy that uses simulated data to identify the conditions under which different methods perform well and applies what is learned from the simulations to predict which method will perform best on never-before-seen empirical data sets. We apply this strategy to a class of methods that group respondents to attitude surveys according to whether they share construals of a given domain. This allows us to identify the relative strengths and weaknesses of the methods we consider, including relational class analysis, correlational class analysis, and eight other such variants. Results support the “no free lunch” view that researchers should abandon the quest for one best algorithm in favor of matching algorithms to kinds of data for which each is most appropriate and provide direction on how to do so.}
}

@article{doi:10.1177/1081286508092613,
author = {S.-Y. Yang and J. Escobar and R.J. Clifton},
title = {Computational Modeling of Stress-Wave-Induced Martensitic Phase Transformations in NiTi},
journal = {Mathematics and Mechanics of Solids},
volume = {14},
number = {1–2},
pages = {220–257},
year = {2009j},
doi = {10.1177/1081286508092613},
URL = {https://doi-org.crai.referencistas.com/10.1177/1081286508092613},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1081286508092613},
abstract = {Computational simulations based on time-dependent Ginzburg—Landau equations are used to model martensitic phase transformations induced in pressure-shear plate impact experiments. Symmetric impact experiments on polycrystalline NiTi plates, and so-called “sandwich impact” experiments on thin polycrystalline NiTi foils sandwiched between two hard plates, are simulated by characterizing each grain as a three-dimensional finite element that can transform to twinned martensite involving twenty-four habit plane variants. The threshold stress at which the transformation occurs is obtained from the amplitude of the leading shear wave in the symmetric impact experiments. The kinetic coefficient in the Ginzburg—Landau equation is obtained by matching the risetime of the transverse particle velocity in the sandwich impact experiments. These simulations highlight the importance of including the effects of self-accommodation in which clusters of several habit plane variants nucleate and grow simultaneously to reduce the constraining effect of the surrounding material.}
}

