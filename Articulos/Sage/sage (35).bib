@article{doi:10.1177/1094342016649245,
author = {F Auricchio and M Ferretti and A Lefieux and M Musci and A Reali and S Trimarchi and A Veneziani},
title = {Parallelizing a finite element solver in computational hemodynamics: A black box approach},
journal = {The International Journal of High Performance Computing Applications},
volume = {32},
number = {3},
pages = {351–362},
year = {2018a},
doi = {10.1177/1094342016649245},
URL = {https://doi-org.crai.referencistas.com/10.1177/1094342016649245},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342016649245},
abstract = {In the last 20 years, a new approach has emerged to investigate the physiopathology of circulation. By merging medical images with validated numerical models, it is possible to support doctors’ decision-making process. The iCardioCloud project aims at establishing a computational framework to perform a complete patient-specific numerical analysis, specially oriented to aortic diseases (like dissections or aneurysms) and to deliver a compelling synthesis. The project can be considered a pioneering example of a Computer Aided Clinical Trial: i.e., a comprehensive analysis of patients where the level of knowledge extracted by traditional measures and statistics is enhanced through the massive use of numerical modeling. From a computer engineering point of view, iCardioCloud faces multiple challenges. First, the number of problems to solve for each patient is significantly huge – this is typical of computational fluid dynamics (CFD) – and it requires parallel methods. In addition, working in a clinical environment demands efficiency as the timeline requires rapid quantitative answers (as may happen in an emergency scenario). It is therefore mandatory to employ high-end parallel systems, such as large clusters or supercomputers. Here we discuss a parallel implementation of an application within the iCardioCloud project, built with a black-box approach – i.e., by assembling and configuring existing packages and libraries and in particular LifeV, a finite element library developed to solve CFD problems. The goal of this paper is to describe the software architecture underlying LifeV and to assess its performance and the most appropriate parallel paradigm. This paper is an extension of a previous work presented at the PBio 2015 Conference. This revision extends the description of the software architecture and discusses several new serial and parallel optimizations to the application. We discuss the introduction of hybrid parallelism in order to mitigate some performance problems previously experienced.}
}

@article{doi:10.1177/0954411914542170,
author = {Fereshteh Bahramian and Hadi Mohammadi},
title = {A novel periodic boundary condition for computational hemodynamics studies},
journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
volume = {228},
number = {7},
pages = {643–651},
year = {2014b},
doi = {10.1177/0954411914542170},
note = {PMID:25015666},
URL = {https://doi-org.crai.referencistas.com/10.1177/0954411914542170},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0954411914542170},
abstract = {In computational fluid dynamics models for hemodynamics applications, boundary conditions remain one of the major issues in obtaining accurate fluid flow predictions. For major cardiovascular models, the realistic boundary conditions are not available. In order to address this issue, the whole computational domain needs to be modeled, which is practically impossible. For simulating fully developed turbulent flows using the large eddy simulation and dynamic numerical solution methods, which are very popular in hemodynamics studies, periodic boundary conditions are suitable. This is mainly because the computational domain can be reduced considerably. In this study, a novel periodic boundary condition is proposed, which is based on mass flow condition. The proposed boundary condition is applied on a square duct for the sake of validation. The mass-based condition was shown to obtain the solution in 15% less time. As such, the mass-based condition has two decisive advantages: first, the solution for a given Reynolds number can be obtained in a single simulation because of the direct specification of the mass flow, and second, simulations can be made more quickly.}
}

@article{doi:10.1177/2056305119896057,
author = {António Filipe Fonseca and Sohhom Bandyopadhyay and Jorge Louçã and Jaison A. Manjaly},
title = {Caste in the News: A Computational Analysis of Indian Newspapers},
journal = {Social Media + Society},
volume = {5},
number = {4},
pages = {2056305119896057},
year = {2019c},
doi = {10.1177/2056305119896057},
URL = {https://doi-org.crai.referencistas.com/10.1177/2056305119896057},
eprint = {https://doi-org.crai.referencistas.com/10.1177/2056305119896057},
abstract = {Conflicts involving caste issues, mainly concerning the lowest caste rights, pervade modern Indian society. Caste affiliation, being rigorously enforced by the society, is an official contemporary reality. Although caste identity is a major social discrimination, it also serves as a necessary condition for affirmative action like reservation policy. In this article, we perform an original and rigorous analysis of the discourse involving the theme “caste” in India newspapers. To this purpose, we have implemented a computational analysis over a big dataset of the 2016 and 2017 editions of three major Indian newspapers to determine the most salient themes associated with “caste” in the news. We have used an original mix of state-of-the-art algorithms, including those based on statistical distributions and two-layer neural networks, to detect the relevant topics in the news and characterize their linguistic context. We concluded that there is an excessive association between lower castes, victimization, and social unrest in the news that does not adequately cover the reports on other aspects of their life and personal identity, thus reinforcing conflict, while attenuating the vocality and agency of a large section of the population. From our conclusion, we propose a positive discrimination policy in the newsroom.}
}

@article{doi:10.1177/00405175211036733,
author = {Weihua Gu and Fuguo Li and Qinchao Gao and Chengzhi Zhuo and Zhong Lu},
title = {Design of double teeth metallic card clothing for the high-efficiency carding process by computational fluid dynamics},
journal = {Textile Research Journal},
volume = {92},
number = {15–16},
pages = {2909–2921},
year = {2022d},
doi = {10.1177/00405175211036733},
URL = {https://doi-org.crai.referencistas.com/10.1177/00405175211036733},
eprint = {https://doi-org.crai.referencistas.com/10.1177/00405175211036733},
abstract = {The design of metallic card clothing, which is one of the most important devices in the textile industry, has always been based on operational experience. With the development of types of fibers and the requirements for the quality of yarns, those principles concluded by engineers seem to be losing their efficiency. Recent research found that airflow played an important role in the card process, which means airflow should be carefully studied. Computational fluid dynamics (CFD) simulation greatly helps in the analysis of airflow because the gauge between carding elements is too narrow to put in any measuring device. In the present study, with the help of CFD simulation, the air around different carding clothing with varied tooth depth was analyzed. It was concluded that the carding efficiency improvement in card clothing with lower tooth depth may be related to more concentrated air velocity at the tooth tips. This resulted in more probabilities that fibers would get through the cylinder surface at the teeth tips, so that the fibers could be caught by flat-top needles more efficiently. With this assumption, a new generation of card clothing called “double teeth” containing two teeth in a single section has been invented. The new configuration design of card clothing was then applied in several spinning mills on an industrial scale for experiments. The results showed about a 30% improvement in production at the same quality level as conventional card clothing, which implied the usefulness of the newly applied principles related to airflow. Despite the difficulty in the study of the complex carding process, the new airflow analysis method has shown an optional and worthwhile way of thinking that could make a difference in future research in the textile industry.}
}

@article{doi:10.1177/1350508408100474,
author = {Jannis Kallinikos},
title = {On the Computational Rendition of Reality: Artefacts and Human Agency},
journal = {Organization},
volume = {16},
number = {2},
pages = {183–202},
year = {2009e},
doi = {10.1177/1350508408100474},
URL = {https://doi-org.crai.referencistas.com/10.1177/1350508408100474},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1350508408100474},
abstract = {The paper seeks to lay open the computational logic by which reality is rendered as information. Computation is claimed to involve a drift away from the palpable and extendible character of things, a trend that both continues and breaks with the prevailing strategies of technological mediation in industrialism and modernity. Computation entails the relentless analytic reduction of the composite character and complexion of the world. Reality is meticulously dissolved and regained after a long analytic retreat and technological reconstruction. The outcome of this analytic strategy is that processes taking place at the human-technology interface are sustained by an elaborate vertical stratification, entailing a variety of other programmes and systems that reach down from the level of the interface to machine language and the mechanics of binary parsing. The deepening involvement of computation in instrumental settings thus reframes the perceptive and action modalities by which human agents confront the world. This way, a coherent set of techniques for building up reality is established accompanied by a new model of human agency that increasingly takes the form of a combinatoria of data and information items, remaking the shape of things out of the digital fragments produced by computation.}
}

@article{doi:10.1177/1538574413503561,
author = {Christof Karmonik and Matthias Müller-Eschner and Sasan Partovi and Philipp Geisbüsch and Maria-Katharina Ganten and Jean Bismuth and Mark G. Davies and Dittmar Böckler and Matthias Loebe and Alan B. Lumsden et al.},
title = {Computational Fluid Dynamics Investigation of Chronic Aortic Dissection Hemodynamics Versus Normal Aorta},
journal = {Vascular and Endovascular Surgery},
volume = {47},
number = {8},
pages = {625–631},
year = {2013f},
doi = {10.1177/1538574413503561},
note = {PMID:24048257},
URL = {https://doi-org.crai.referencistas.com/10.1177/1538574413503561},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1538574413503561},
abstract = {Objectives: To evaluate hemodynamic changes during aneurysmal dilatation in chronic type B aortic dissections compared to hemodynamic parameters in the healthy aorta with the use of computational fluid dynamics (CFD). Methods: True lumen (TL)/false lumen (FL) dimensional changes, changes in total pressure (TP), and wall shear stress (WSS) were evaluated at follow-up (FU) compared to initial examination (IE) with transient CFD simulation with geometries derived from clinical image data and inflow boundary conditions from magnetic resonance images. The TL/FL pressure gradient between ascending and descending aorta (DAo) and maximum WSS at the site of largest dilatation was compared to values for the healthy aorta. Results: Hemodynamic changes at site of largest FL dilatation included 77% WSS reduction and 69% TP reduction. Compared to the healthy aorta, pressure gradient between ascending and DAo was a factor of 1.4 higher in the TL and a factor of 1.5 in the FL and increased at FU (1.6 and 1.7, respectively). Maximum WSS at the site of largest dilatation was a factor of 3 lower than that for the healthy aorta at IE and decreased by more than a factor of 2 at FU. Conclusions: The FL dilatation at FU favorably reduced TP. In contrast, unfavorable increase in pressure gradient between ascending and DAo was observed with higher values than in the healthy aorta. Maximum WSS was reduced at the site of largest dilation compared to healthy aorta.}
}

@article{doi:10.1177/2053951720949571,
author = {Noortje Marres},
title = {For a situational analytics: An interpretative methodology for the study of situations in computational settings},
journal = {Big Data & Society},
volume = {7},
number = {2},
pages = {2053951720949571},
year = {2020g},
doi = {10.1177/2053951720949571},
URL = {https://doi-org.crai.referencistas.com/10.1177/2053951720949571},
eprint = {https://doi-org.crai.referencistas.com/10.1177/2053951720949571},
abstract = {This article introduces an interpretative approach to the analysis of situations in computational settings called situational analytics. I outline the theoretical and methodological underpinnings of this approach, which is still under development, and show how it can be used to surface situations from large data sets derived from online platforms such as YouTube. Situational analytics extends to computationally-mediated settings a qualitative methodology developed by Adele Clarke, Situational Analysis (2005), which uses data mapping to detect heterogeneous entities in fieldwork data to determine ‘what makes a difference’ in a situation. Situational analytics scales up this methodology to analyse situations latent in computational data sets with semi-automated methods of textual and visual analysis. I discuss how this approach deviates from recent analyses of situations in computational social science, and argue that Clarke’s framework renders tractable a fundamental methodological problem that arises in this area of research: while social researchers turn to computational settings in order to analyse social life, the social processes unfolding in these envirnoments are fundamentally affected by the computational architectures in which they occur. Situational analytics offers a way to address this problematic by making a heterogeneously composed situation – involving social, technical and media elements – the unit of computational analysis. To conclude, I show how situational analytics can be applied in a case study of YouTube videos featuring intelligent vehicles and discuss how situational analysis itself needs to be elaborated if we are to come to terms with computational transformations of the situational fabric of social life.}
}

@article{doi:10.1080/09506608.2023.2169501,
author = {Shashank Sharma and Sameehan S. Joshi and Mangesh V. Pantawane and Madhavan Radhakrishnan and Sangram Mazumder and Narendra B. Dahotre},
title = {Multiphysics multi-scale computational framework for linking process–structure–property relationships in metal additive manufacturing: a critical review},
journal = {International Materials Reviews},
volume = {68},
number = {7},
pages = {943–1009},
year = {2023h},
doi = {10.1080/09506608.2023.2169501},
URL = {https://doi-org.crai.referencistas.com/10.1080/09506608.2023.2169501},
eprint = {https://doi-org.crai.referencistas.com/10.1080/09506608.2023.2169501},
abstract = {This review article provides a critical assessment of the progress made in computational modelling of metal-based additive manufacturing (AM) with emphasis on its ability to predict physical phenomena, concepts of microstructural evolution, residual stresses, role of multiple thermal cycles, and formation of multi-dimensional defects along with the achieved degree of experimental validation. The uniqueness of this article stems from the inclusion of comprehensive information on computational progress in the field of fusion-based, sintering-based, and mechanical deformation-based AM. A computational model’s role in determining the process framework for the desired outcome of the set properties of the AM components is recognised while presenting the process-microstructure maps, thereby appraising computational ability towards the qualification of products. The inclusion of a detailed discussion on the bi-directional coupling of machine learning and physics-based computational models provides a futuristic roadmap for the digital twin of metal-based AM.}
}

@article{doi:10.1177/1029864918757595,
author = {Christof Weiß and Matthias Mauch and Simon Dixon and Meinard Müller},
title = {Investigating style evolution of Western classical music: A computational approach},
journal = {Musicae Scientiae},
volume = {23},
number = {4},
pages = {486–507},
year = {2019i},
doi = {10.1177/1029864918757595},
URL = {https://doi-org.crai.referencistas.com/10.1177/1029864918757595},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1029864918757595},
abstract = {In musicology, there has been a long debate about a meaningful partitioning and description of music history regarding composition styles. Particularly, concepts of historical periods have been criticized since they cannot account for the continuous and interwoven evolution of style. To systematically study this evolution, large corpora are necessary suggesting the use of computational strategies. This article presents such strategies and experiments relying on a dataset of 2000 audio recordings, which cover more than 300 years of music history. From the recordings, we extract different tonal features. We propose a method to visualize these features over the course of history using evolution curves. With the curves, we re-trace hypotheses concerning the evolution of chord transitions, intervals, and tonal complexity. Furthermore, we perform unsupervised clustering of recordings across composition years, individual pieces, and composers. In these studies, we found independent evidence of historical periods that broadly agrees with traditional views as well as recent data-driven experiments. This shows that computational experiments can provide novel insights into the evolution of styles.}
}

@article{doi:10.1177/0963721420915873,
author = {Johannes C. Ziegler and Conrad Perry and Marco Zorzi},
title = {Learning to Read and Dyslexia: From Theory to Intervention Through Personalized Computational Models},
journal = {Current Directions in Psychological Science},
volume = {29},
number = {3},
pages = {293–300},
year = {2020j},
doi = {10.1177/0963721420915873},
note = {PMID:32655213},
URL = {https://doi-org.crai.referencistas.com/10.1177/0963721420915873},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0963721420915873},
abstract = {How do children learn to read? How do deficits in various components of the reading network affect learning outcomes? How does remediating one or several components change reading performance? In this article, we summarize what is known about learning to read and how this can be formalized in a developmentally plausible computational model of reading acquisition. The model is used to understand normal and impaired reading development (dyslexia). In particular, we show that it is possible to simulate individual learning trajectories and intervention outcomes on the basis of three component skills: orthography, phonology, and vocabulary. We therefore advocate a multifactorial computational approach to understanding reading that has practical implications for dyslexia and intervention.}
}

